{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training Validation Dataset Splitter"
      ],
      "metadata": {
        "id": "1MzpsrMD55Lm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code was created to split the Georgia Coastal Survey 2022 dataset into training and validation sets for Digital Elevation Model correction. \n",
        "\n",
        "This will work to split any .csv file into training and validation data, but the example carried throughout was to conduct a stratified split by vegetation class abundance."
      ],
      "metadata": {
        "id": "QAeR85hD5mb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, import the necessary modules."
      ],
      "metadata": {
        "id": "goiEPn3cFcgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ITe2Wy4EF-xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Allow google colab to import files from your computer. After this command is run, it will allow you to choose the file from your computer directory that you wish to use. In this case, I am using a csv from an ESRI Shapefile that I created from our RTK data."
      ],
      "metadata": {
        "id": "2bpwOhyBy6fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "qd2WGdmFGJNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the csv"
      ],
      "metadata": {
        "id": "Hu5apTr_zMLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(io.StringIO(uploaded['Data_V2_All_2022.csv'].decode('utf-8')))\n",
        "df"
      ],
      "metadata": {
        "id": "GpHTbEscGyxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the desired x and y variables. In this case, I want the plot number and the vegetation columns from the table."
      ],
      "metadata": {
        "id": "nTkbqri540u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.Plot\n",
        "y = df.Dominant"
      ],
      "metadata": {
        "id": "wTH3BRFyL6_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(x,y)"
      ],
      "metadata": {
        "id": "yf8wQ1makqDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'train_test_split' tool from sklearn can be used to split our dataset into training and validation sets. Additionally, the stratify variable will allow us to stratify our data according to abundance of our veg catagory. The random state variable sets the seed to insure we have the same split with re-runs of this code, and the test size variable sets the data split by the validation size. The training set data size will be (1 - test_size). For example, I would set \"test_size\" to 0.3 for a 70/30 Training/Validation split.\n",
        "\n",
        "If you dont need to stratify your dataset, the stratify call can simply be deleted."
      ],
      "metadata": {
        "id": "hP1WHqSN49dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.3, random_state=71, stratify=y)"
      ],
      "metadata": {
        "id": "0w1CreshH2oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('x_train : ')\n",
        "print(x_train.head())\n",
        "print(x_train.shape)\n",
        "print('')\n",
        "print('x_test : ')\n",
        "print(x_test.head())\n",
        "print(x_test.shape)\n",
        "print('')\n",
        "print('y_train : ')\n",
        "print(y_train.head())\n",
        "print(y_train.shape)\n",
        "print('')\n",
        "print('y_test : ')\n",
        "print(y_test.head())\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "GNr10QD8MYTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, I want to export a .csv of plot names (my x variable) for my training and validation datasets. The following code will export and download these as a .csv file."
      ],
      "metadata": {
        "id": "BswvDsyt3FsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(x_train)\n",
        "np.array(x_test)\n",
        "np.array(y_train)\n",
        "np.array(y_test)"
      ],
      "metadata": {
        "id": "agN3c3XvP2xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataxTrain = np.array(x_train)\n",
        "dataxTest = np.array(x_test)\n",
        "dataytrain = np.array(y_train)\n",
        "dataytest = np.array(y_test)"
      ],
      "metadata": {
        "id": "Z00pPx0cTqpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ExportxTraining = pd.DataFrame(x_train)\n",
        "ExportxTraining\n",
        "ExportxTest = pd.DataFrame(x_test)\n",
        "ExportxTest"
      ],
      "metadata": {
        "id": "E75nnWNTAhXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the following block to set the filename for your .csv file. "
      ],
      "metadata": {
        "id": "O7eTWkQl3WFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ExportxTraining.to_csv('TrainingTest1_3oct22.csv')\n",
        "ExportxTest.to_csv('xTest_Test1_5oct22.csv')"
      ],
      "metadata": {
        "id": "jtEG9-AWCBRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('TrainingTest1_3oct22.csv')\n",
        "files.download('xTest_Test1_5oct22.csv')"
      ],
      "metadata": {
        "id": "5K2ZS_QdCHAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Created by T.Pudil for use in the Hladik lab at Georgia Southern University, Department of Geography"
      ],
      "metadata": {
        "id": "fGCTPQWA6AQ-"
      }
    }
  ]
}